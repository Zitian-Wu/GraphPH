{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2018f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Toy connectome-like demo using graphph.h0h1_connectome\n",
    "\n",
    "Pipeline:\n",
    "  0) Simulate 3 classes × 3 subjects of connectome-like distance matrices\n",
    "  1) Save raw distances to toy_connectome_demo/raw_D with index.csv + meta.json\n",
    "  2) Build H0/H1 features via build_features_for_sharded\n",
    "  3) Take one subject, compute VR persistence + barcode plot\n",
    "  4) Run edgewise MLE (run_and_save_all_mle)\n",
    "  5) Run hierarchical NUTS (run_and_save_all_hierarchical)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import graphph.h0h1_connectome as gpc\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0. Simulate toy \"connectome-like\" distance shards on disk\n",
    "# ---------------------------------------------------------------------\n",
    "ROOT = Path(\"toy_connectome_demo\").resolve()\n",
    "RAW_ROOT = ROOT / \"raw_D\"\n",
    "FEAT_ROOT = ROOT / \"feat_h0h1\"\n",
    "RUN_MLE_ROOT = ROOT / \"runs_mle\"\n",
    "RUN_HMC_ROOT = ROOT / \"runs_hmc\"\n",
    "\n",
    "for p in [RAW_ROOT, FEAT_ROOT, RUN_MLE_ROOT, RUN_HMC_ROOT]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "n_roi = 10\n",
    "n_classes = 3\n",
    "n_subj_per_class = 3\n",
    "\n",
    "class_ids = [f\"Group{i}\" for i in range(1, n_classes + 1)]\n",
    "\n",
    "# Layout ROIs on a circle in 2D\n",
    "theta = np.linspace(0, 2 * np.pi, n_roi, endpoint=False)\n",
    "coords = np.vstack([np.cos(theta), np.sin(theta)]).T  # (n_roi, 2)\n",
    "\n",
    "\n",
    "def base_connectivity(coords, length_scale=0.7):\n",
    "    \"\"\"Simple connectivity strength decaying with Euclidean distance.\"\"\"\n",
    "    n = coords.shape[0]\n",
    "    W = np.zeros((n, n), dtype=float)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            d = np.linalg.norm(coords[i] - coords[j])\n",
    "            w_ij = np.exp(-d / length_scale)  # larger for closer ROIs\n",
    "            W[i, j] = W[j, i] = w_ij\n",
    "    return W\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Class-level connectivity templates\n",
    "# -----------------------------\n",
    "W_base = base_connectivity(coords, length_scale=0.7)\n",
    "\n",
    "# Define three rough \"modules\"\n",
    "clusterA = np.array([0, 1, ])\n",
    "clusterB = np.array([3, 4, 5])\n",
    "clusterC = np.array([6, 7, 8, 9])\n",
    "\n",
    "W_templates = {}\n",
    "\n",
    "# Group1: stronger within A and B (modular pattern)\n",
    "W1 = W_base.copy()\n",
    "W1[np.ix_(clusterA, clusterA)] *= 3.5\n",
    "W1[np.ix_(clusterB, clusterB)] *= 3.5\n",
    "W_templates[\"Group1\"] = W1\n",
    "\n",
    "# Group2: weaker A/A & B/B, but stronger A–B cross-talk\n",
    "W2 = W_base.copy()\n",
    "W2[np.ix_(clusterA, clusterA)] *= 0.3\n",
    "W2[np.ix_(clusterB, clusterB)] *= 0.3\n",
    "W2[np.ix_(clusterA, clusterB)] *= 3.0\n",
    "W2[np.ix_(clusterB, clusterA)] *= 3.0\n",
    "W_templates[\"Group2\"] = W2\n",
    "\n",
    "# Group3: strong C cluster + stronger long-range edges\n",
    "W3 = W_base.copy()\n",
    "W3[np.ix_(clusterC, clusterC)] *= 3.0\n",
    "# \"Far\" edges = pairs with large Euclidean distance on the circle\n",
    "dist_mat = np.linalg.norm(coords[:, None, :] - coords[None, :, :], axis=-1)\n",
    "far_mask = dist_mat > 1.6\n",
    "W3[far_mask] *= 2.0\n",
    "W_templates[\"Group3\"] = W3\n",
    "\n",
    "# -----------------------------\n",
    "# Convert templates to distance and add subject-level noise\n",
    "# -----------------------------\n",
    "eps = 1e-3\n",
    "subj_noise_sd = 0.01     # within-class variability\n",
    "\n",
    "index_rows = []\n",
    "global_eps_max = 0.0\n",
    "\n",
    "for cid in class_ids:\n",
    "    class_dir = RAW_ROOT / cid\n",
    "    class_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    W_template = W_templates[cid]\n",
    "\n",
    "    # --- NEW: Laplacian-Eigenmaps distance instead of 1/(W+eps) ---\n",
    "    # For toy data, we set min_weight=None so we don't threshold edges by 20.\n",
    "    D_template, le_info = gpc.weights_to_le_distance(\n",
    "        W_template,\n",
    "        k=5,              # same as your real pipeline\n",
    "        min_weight=None,  # IMPORTANT: no fiber-count threshold for toy W\n",
    "        sym=\"avg\",\n",
    "        transform=\"log1p\",\n",
    "        rescale=True,\n",
    "    )\n",
    "    # D_template is already symmetric with 0 diagonal by construction.\n",
    "\n",
    "    for k_subj in range(n_subj_per_class):\n",
    "        # multiplicative symmetric noise around the class template distance\n",
    "        noise = rng.normal(loc=0.0, scale=subj_noise_sd, size=(n_roi, n_roi))\n",
    "        noise = 0.5 * (noise + noise.T)\n",
    "        np.fill_diagonal(noise, 0.0)\n",
    "\n",
    "        D = D_template * (1.0 + noise)\n",
    "        D = np.clip(D, 0.0, None)\n",
    "\n",
    "        finite = np.isfinite(D)\n",
    "        subj_eps_max = float(np.max(D[finite]))\n",
    "        subj_cap = subj_eps_max + 0.5\n",
    "        global_eps_max = max(global_eps_max, subj_eps_max)\n",
    "\n",
    "        rel_path = f\"{cid}/subj_{k_subj:03d}.npz\"\n",
    "        shard_path = RAW_ROOT / rel_path\n",
    "\n",
    "        np.savez_compressed(\n",
    "            shard_path,\n",
    "            D=D.astype(np.float32),\n",
    "            eps_max=np.array(subj_eps_max, dtype=np.float32),\n",
    "            cap=np.array(subj_cap, dtype=np.float32),\n",
    "        )\n",
    "\n",
    "        index_rows.append(\n",
    "            {\n",
    "                \"cid\": cid,\n",
    "                \"file\": rel_path,\n",
    "                \"subject\": f\"{cid}_subj_{k_subj:03d}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Write index.csv expected by build_features_for_sharded\n",
    "index_path = RAW_ROOT / \"index.csv\"\n",
    "with index_path.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"cid\", \"file\", \"subject\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(index_rows)\n",
    "\n",
    "# Optional: meta.json with global VR distance info\n",
    "meta = {\n",
    "    \"distance_info\": {\n",
    "        \"vr\": {\n",
    "            \"eps_max\": float(global_eps_max),\n",
    "            \"cap\": float(global_eps_max + 0.5),\n",
    "        }\n",
    "    },\n",
    "    \"n_roi\": n_roi,\n",
    "    \"class_ids\": class_ids,\n",
    "}\n",
    "(RAW_ROOT / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "print(f\"[ok] wrote raw shards + index.csv to {RAW_ROOT}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Build H0/H1 features from raw shards\n",
    "# ---------------------------------------------------------------------\n",
    "gpc.build_features_for_sharded(\n",
    "    base_dir=str(RAW_ROOT),\n",
    "    out_dir=str(FEAT_ROOT),\n",
    "    divide_by_two=True,   # VR radius-time convention, like your real pipeline\n",
    "    prefer_filled=False,  # keep +inf semantics if there were any\n",
    "    prefer_vr=False,      # we only stored D; no D_vr here\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "print(f\"[ok] feature files written to {FEAT_ROOT}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Pick one subject and visualize VR persistence (barcode)\n",
    "# ---------------------------------------------------------------------\n",
    "# Just grab the first Group1 shard we created\n",
    "example_npz = sorted((RAW_ROOT / \"Group1\").glob(\"subj_*.npz\"))[0]\n",
    "with np.load(example_npz) as z:\n",
    "    D_example = z[\"D\"]\n",
    "\n",
    "st, persistence, max_edge = gpc.vr_persistence_from_distance(\n",
    "    D_example,\n",
    "    max_dim=2,\n",
    ")\n",
    "\n",
    "print(f\"[info] example VR: max_edge = {max_edge:.4f}\")\n",
    "\n",
    "# One stacked figure with all dimensions (0,1,2) as in your snippet\n",
    "gpc.plot_all_dims_barcode(persistence, use_latex=False)\n",
    "plt.suptitle(\"Toy connectome: VR barcodes for one subject (Group1)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. MLE run (edgewise, no prior/Jacobian) using run_and_save_all_mle\n",
    "# ---------------------------------------------------------------------\n",
    "MLE_m = 5  # same as in your real FitConfig\n",
    "\n",
    "class_Lambda_mle, C_mle, acc_mle = gpc.run_and_save_all_mle(\n",
    "    feat_dir=str(FEAT_ROOT),\n",
    "    out_root=str(RUN_MLE_ROOT),\n",
    "    m=MLE_m,\n",
    "    use_h1=True,\n",
    "    init=\"zeros\",      # or \"nuts_mean\" if you later want warm-starts\n",
    "    maxiter=1000,      # keep it smaller for toy demo\n",
    "    gtol=1e-6,\n",
    ")\n",
    "\n",
    "print(\"\\n[ok] finished MLE on toy data\")\n",
    "print(\"  class_Lambda_mle type:\", type(class_Lambda_mle))\n",
    "print(\"  C_mle type:\", type(C_mle))\n",
    "print(\"  acc_mle:\", acc_mle)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Hierarchical NUTS run using run_and_save_all_hierarchical\n",
    "# ---------------------------------------------------------------------\n",
    "# You can either:\n",
    "#   - use zeros / defaults inside the function, or\n",
    "#   - warm-start with MLE estimates as in your real pipeline.\n",
    "#\n",
    "# For the toy example we'll call the simplest version; if your actual\n",
    "# function requires lambda0_by_class / bar_lambda0_init, uncomment the\n",
    "# relevant lines and compute them from class_Lambda_mle.\n",
    "\n",
    "fitcfg = gpc.FitConfig(\n",
    "    m=MLE_m,\n",
    "    kappa=6.0,\n",
    "    kappa0=1.0,\n",
    "    alpha=0.2,\n",
    "    num_warmup=500,\n",
    "    num_samples=2000,   # smaller than your real run\n",
    "    num_chains=1,\n",
    "    target_accept=0.8,\n",
    "    dense_mass=False,\n",
    "    seed=2025,\n",
    ")\n",
    "\n",
    "# OPTIONAL warm starts from MLE (uncomment if your function expects them)\n",
    "# mle_Lambdas = class_Lambda_mle  # whatever structure your code uses\n",
    "# bar_lambda0 = np.mean(mle_Lambdas, axis=0)\n",
    "\n",
    "hmc_out = gpc.run_and_save_all_hierarchical(\n",
    "    feat_dir=str(FEAT_ROOT),\n",
    "    out_root=str(RUN_HMC_ROOT),\n",
    "    fitcfg=fitcfg,\n",
    "    class_ids=None,        # use all classes found in index.csv\n",
    "    use_h1=True,\n",
    "    # If your version *requires* these, uncomment and adapt:\n",
    "    # lambda0_by_class=mle_Lambdas,\n",
    "    # bar_lambda0_init=bar_lambda0,\n",
    "    # phi_diag_indices_bar=[(0, 0), (10, 0)],\n",
    "    # phi_diag_indices_each=[(0, 0), (10, 0)],\n",
    "    max_lag=50,\n",
    "    thin_every_for_diag=1,\n",
    "    save_per_class_phi=True,\n",
    ")\n",
    "\n",
    "print(\"\\n[ok] finished hierarchical HMC on toy data\")\n",
    "print(\"  hmc_out type:\", type(hmc_out))\n",
    "try:\n",
    "    print(\"  hmc_out keys:\", list(hmc_out.keys()))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 5. Confusion matrix from HMC posterior mean Λ\n",
    "# ---------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Try to get class → Lambda_bar mapping from the returned object\n",
    "class_Lambda_hmc = {}\n",
    "\n",
    "if isinstance(hmc_out, dict):\n",
    "    # Case 1: run_and_save_all_hierarchical already returns a dict of Λ's\n",
    "    if \"class_Lambda_bar\" in hmc_out:\n",
    "        # e.g. {\"Group1\": Λ_bar1, \"Group2\": Λ_bar2, ...}\n",
    "        class_Lambda_hmc = hmc_out[\"class_Lambda_bar\"]\n",
    "    elif \"class_Lambda\" in hmc_out:\n",
    "        class_Lambda_hmc = hmc_out[\"class_Lambda\"]\n",
    "    else:\n",
    "        # Sometimes the function *itself* just returns {cid: Λ_bar}\n",
    "        # If the values look like arrays, treat hmc_out as the mapping\n",
    "        if all(isinstance(v, np.ndarray) for v in hmc_out.values()):\n",
    "            class_Lambda_hmc = hmc_out\n",
    "\n",
    "# If that didn’t work, fall back to loading per-class NPZs from disk\n",
    "if not class_Lambda_hmc:\n",
    "    class_Lambda_hmc = {}\n",
    "    for cid in class_ids:\n",
    "        # Adjust these filenames / keys to match what your code actually saves\n",
    "        npz_bar = RUN_HMC_ROOT / cid / \"Lambda_bar.npz\"\n",
    "        npz_hat = RUN_HMC_ROOT / cid / \"Lambda_hat.npz\"\n",
    "\n",
    "        if npz_bar.exists():\n",
    "            with np.load(npz_bar) as z:\n",
    "                # try common dataset names\n",
    "                if \"Lambda_bar\" in z.files:\n",
    "                    class_Lambda_hmc[cid] = z[\"Lambda_bar\"]\n",
    "                elif \"Lambda_hat\" in z.files:\n",
    "                    class_Lambda_hmc[cid] = z[\"Lambda_hat\"]\n",
    "                else:\n",
    "                    raise KeyError(f\"No Lambda_* in {npz_bar}\")\n",
    "        elif npz_hat.exists():\n",
    "            with np.load(npz_hat) as z:\n",
    "                if \"Lambda_hat\" in z.files:\n",
    "                    class_Lambda_hmc[cid] = z[\"Lambda_hat\"]\n",
    "                elif \"Lambda_bar\" in z.files:\n",
    "                    class_Lambda_hmc[cid] = z[\"Lambda_bar\"]\n",
    "                else:\n",
    "                    raise KeyError(f\"No Lambda_* in {npz_hat}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find posterior Λ file for class {cid} \"\n",
    "                f\"in {RUN_HMC_ROOT/cid}\"\n",
    "            )\n",
    "\n",
    "print(\"\\n[info] HMC posterior mean Λ loaded for classes:\")\n",
    "for cid, Lam in class_Lambda_hmc.items():\n",
    "    print(f\"  {cid}: Λ shape = {Lam.shape}\")\n",
    "\n",
    "# Now compute confusion matrix using *HMC* Λ's instead of MLE ones\n",
    "C_hmc, cls_hmc, acc_hmc = gpc.save_confusion_from_feats(\n",
    "    feat_dir=str(FEAT_ROOT),\n",
    "    class_Lambda=class_Lambda_hmc,\n",
    "    out_png=RUN_HMC_ROOT / \"confusion_hmc.png\",\n",
    "    out_csv=RUN_HMC_ROOT / \"confusion_hmc.csv\",\n",
    ")\n",
    "\n",
    "print(\"\\n[ok] HMC-based confusion\")\n",
    "print(\"classes order:\", cls_hmc)\n",
    "print(\"confusion matrix:\\n\", C_hmc)\n",
    "print(\"HMC accuracy:\", acc_hmc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fa19a",
   "metadata": {},
   "source": [
    "# Latent postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73487a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from graphph.latent_postproc import (\n",
    "    plot_latent_coords_panels,\n",
    "    latent_violin_from_samples,\n",
    "    fdr_select_rois_from_latentdiff,\n",
    "    replot_latent_violin_from_npz,\n",
    ")\n",
    "\n",
    "RUN_DIR = Path(RUN_HMC_ROOT)\n",
    "\n",
    "# latent coords panels\n",
    "plot_latent_coords_panels(\n",
    "    RUN_DIR,\n",
    "    groups=[\"Group1\", \"Group2\", \"Group3\"],\n",
    "    align_to=\"Group2\",\n",
    "    out_dir=\"figs/latent_coords\",\n",
    "    x_stretch=1.01,\n",
    ")\n",
    "\n",
    "# save per-draw diffs for each pair\n",
    "pairs = [(\"Group1\", \"Group2\"), (\"Group1\", \"Group3\"), (\"Group2\", \"Group3\")]\n",
    "for A, B in pairs:\n",
    "    latent_violin_from_samples(\n",
    "        RUN_DIR,\n",
    "        pair=(A, B),\n",
    "        rank=5,\n",
    "        ref_label=\"Group2\",\n",
    "        node_csv=\"data/node_labels.csv\",\n",
    "        top_k=None,\n",
    "        sort=False,\n",
    "        color=\"C0\",\n",
    "        label_map={\"Group1\": \"Group1\", \"Group2\": \"Group2\", \"Group3\": \"Group3\"},\n",
    "        save_draw_diffs_dir=\"tabs/fdr_latent_diffs\",\n",
    "    )\n",
    "\n",
    "Path(\"tabs/fdr_latent_diffs\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"figs/roi_violin\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pairs = [\n",
    "    (\"Group1\", \"Group2\"),\n",
    "    (\"Group1\", \"Group3\"),\n",
    "    (\"Group2\", \"Group3\"),\n",
    "]\n",
    "\n",
    "for A, B in pairs:\n",
    "    print(f\"\\n=== FDR + violin for {A} vs {B} ===\")\n",
    "\n",
    "    # .npz with per-draw latent diffs – already created by latent_violin_from_samples\n",
    "    npz_path = f\"tabs/fdr_latent_diffs/latentdiff_draws_rank5_{A}_vs_{B}.npz\"\n",
    "\n",
    "    # pair-specific FDR CSV\n",
    "    fdr_csv = f\"tabs/fdr_latent_diffs/fdr_selected_rois_rank5_{A}_vs_{B}.csv\"\n",
    "\n",
    "    # 1) run FDR for this pair\n",
    "    fdr_out = fdr_select_rois_from_latentdiff(\n",
    "        npz_path,\n",
    "        beta=0.9,\n",
    "        out_csv=fdr_csv,\n",
    "    )\n",
    "\n",
    "    # 2) manuscript-style violin replot with FDR labels for this pair\n",
    "    png_path = f\"figs/roi_violin/violin_rank5_{A}_vs_{B}_manuscript.png\"\n",
    "\n",
    "    replot_latent_violin_from_npz(\n",
    "        npz_path=npz_path,\n",
    "        top_k=None,              # all ROIs\n",
    "        sort=False,\n",
    "        fig_width=6.0,\n",
    "        fig_height=3.4,\n",
    "        fdr_csv=fdr_csv,         # use the pair-specific FDR file\n",
    "        annotate_fdr_points=True,\n",
    "        png_path=png_path,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ffbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6dc9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4be009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tda_neuroimaging @ HPG)",
   "language": "python",
   "name": "tda_neuroimaging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
